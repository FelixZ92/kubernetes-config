---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: prometheus-operator
  namespace: observabilty
  annotations:
    fluxcd.io/automated: "false"
spec:
  releaseName: prometheus-operator
  chart:
    repository: https://kubernetes-charts.storage.googleapis.com
    name: prometheus-operator
    version: 8.5.4
  values:
    commonLabels:
      deployment: provisioning

    defaultRules:
      labels:
        deployment: provisioning

    global:
      rbac:
        create: true
        pspEnabled: true

    alertmanager:
      enabled: true
      alertmanagerSpec:
        image:
          repository: quay.io/prometheus/alertmanager
          tag: v0.17.0
        logLevel: info
        spec:
        replicas: 1
        retention: 120h
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: local-storage
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 2700Mi

        externalUrl: "https://alertmanager.192.168.0.14.xip.io"
        routePrefix: /
        resources:
          requests:
            memory: "400Mi"
            cpu: "100m"
          limits:
            memory: "600Mi"
            cpu: "200m"

    ## Using default values from https://github.com/helm/charts/blob/master/stable/grafana/values.yaml
    ##
    grafana:
      enabled: true
      defaultDashboardsEnabled: true
      adminPassword: "123456"

      env:
        GF_AUTH_GENERIC_OAUTH_ENABLED: true
        GF_AUTH_GENERIC_OAUTH_CLIENT_ID: grafana
        GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET: "nop"
        GF_AUTH_GENERIC_OAUTH_SCOPE: openid k8s-dev email profile
        GF_AUTH_GENERIC_OAUTH_AUTH_URL: "https://keycloak.192.168.0.14.xip.io/realms/k8s/protocol/openid-connect/auth"
        GF_AUTH_GENERIC_OAUTH_TOKEN_URL: "https://keycloak.192.168.0.14.xip.io/realms/k8s/protocol/openid-connect/token"
        GF_AUTH_GENERIC_OAUTH_API_URL: "https://keycloak.192.168.0.14.xip.io/realms/k8s/protocol/openid-connect/userinfo"
        GF_SERVER_ROOT_URL: "https://grafana.192.168.0.14.xip.io"
        GF_SERVER_DOMAIN: "grafana.192.168.0.14.xip.io"

      tlsConfig:
        serverName: kubernetes
        insecureSkipVerify: false
      persistence:
        enabled: true
        storageClassName: nfs-client
      ingress:
        enabled: false
        annotations:
          kubernetes.io/ingress.class: traefik-system
        path: /
        hosts:
          - "grafana.192.168.0.14.xip.io"

    ## Component scraping the kubelet and kubelet-hosted cAdvisor
    ##
    kubelet:
      enabled: true
      namespace: kube-system

    ## Component scraping the kube controller manager
    ##
    kubeControllerManager:
      enabled: true

      ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on
      ##
      endpoints:
      # {% for host in groups['controlplane'] %}
      #     - {{ hostvars[host]['ip'] }}
      # {% endfor %}
      # - 10.141.4.23
      # - 10.141.4.24

      ## If using kubeControllerManager.endpoints only the port and targetPort are used
      ##
      service:
        port: 10252
        targetPort: 10252
        selector:
          component: kube-controller-manager

    ## Component scraping coreDns. Use either this or kubeDns
    ##
    coreDns:
      enabled: false
      service:
        port: 9153
        targetPort: 9153
        selector:
          k8s-app: kube-dns

    ## Component scraping kubeDns. Use either this or coreDns
    ##
    kubeDns:
      enabled: true
      service:
        selector:
          k8s-app: kube-dns

    ## Component scraping etcd
    ##
    kubeEtcd:
      enabled: true

      ## If your etcd is not deployed as a pod, specify IPs it can be found on
      ##
      endpoints:
      # {% for host in groups['controlplane'] %}
      #     - {{ hostvars[host]['ip'] }}
      # {% endfor %}
      # - 10.141.4.23
      # - 10.141.4.24

      ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used
      ##
      service:
        port: 2379
        targetPort: 2379
        selector:
          component: etcd

      ## Configure secure access to the etcd cluster by loading a secret into prometheus and
      ## specifying security configuration below. For example, with a secret named etcd-client-cert
      ##
      ## serviceMonitor:
      ##   scheme: https
      ##   insecureSkipVerify: false
      ##   serverName: localhost
      ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca
      ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client
      ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
      ##

    ## Component scraping kube scheduler
    ##
    kubeScheduler:
      enabled: true

      ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on
      ##
      endpoints:
      # {% for host in groups['controlplane'] %}
      #     - {{ hostvars[host]['ip'] }}
      # {% endfor %}
      # - 10.141.4.23
      # - 10.141.4.24

      ## If using kubeScheduler.endpoints only the port and targetPort are used
      ##
      service:
        port: 10251
        targetPort: 10251
        selector:
          component: kube-scheduler

      serviceMonitor:
        ## Scrape interval. If not set, the Prometheus default scrape interval is used.
        ##
        interval: ""
        ## Enable scraping kube-controller-manager over https.
        ## Requires proper certs (not self-signed) and delegated authentication/authorization checks
        ##
        https: false

        ## 	metric relabel configs to apply to samples before ingestion.
        ##
        metricRelabelings: []
        # - action: keep
        #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
        #   sourceLabels: [__name__]

        # 	relabel configs to apply to samples before ingestion.
        ##
        relabelings: []
        # - sourceLabels: [__meta_kubernetes_pod_node_name]
        #   separator: ;
        #   regex: ^(.*)$
        #   target_label: nodename
        #   replacement: $1
        #   action: replace

    ## Component scraping kube state metrics
    ##
    kubeStateMetrics:
      enabled: true

    ## Configuration for kube-state-metrics subchart
    ##
    kube-state-metrics:
      rbac:
        create: true
      podSecurityPolicy:
        enabled: true

    ## Deploy node exporter as a daemonset to all nodes
    ##
    nodeExporter:
      enabled: true
    prometheus-node-exporter:
      tolerations:
        - effect: NoSchedule
          operator: Exists
        - effect: NoExecute
          operator: Exists

    prometheusOperator:
      enabled: true
      createCustomResource: false
      serviceAccount:
        create: true
      kubeletService:
        enabled: true
        namespace: kube-system
      resources:
        limits:
          cpu: 200m
          memory: 200Mi
        requests:
          cpu: 100m
          memory: 100Mi
      image:
        repository: quay.io/coreos/prometheus-operator
        tag: v0.30.1
      configmapReloadImage:
        repository: quay.io/coreos/configmap-reload
        tag: v0.0.1
      prometheusConfigReloaderImage:
        repository: quay.io/coreos/prometheus-config-reloader
        tag: v0.30.1
      hyperkubeImage:
        repository: k8s.gcr.io/hyperkube
        tag: v1.12.1

    prometheus:
      enabled: true
      serviceAccount:
        create: true

      prometheusSpec:
        image:
          repository: quay.io/prometheus/prometheus
          tag: v2.9.1
        externalUrl: "https://prometheus.192.168.0.14.xip.io"
        nodeSelector: {}
        retention: 10d
        replicas: 1
        logLevel: info
        logFormat: logfmt
        resources:
          requests:
            memory: "1000Mi"
            cpu: "200m"
          limits:
            memory: "2000Mi"
            cpu: "400m"

        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: local-storage
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 2700Mi


      additionalServiceMonitors: []
        ## Name of the ServiceMonitor to create
        ##
        # - name: ""

        ## Additional labels to set used for the ServiceMonitorSelector. Together with standard labels from
        ## the chart
        ##
        # additionalLabels: {}

        ## Service label for use in assembling a job name of the form <label value>-<port>
        ## If no label is specified, the service name is used.
        ##
        # jobLabel: ""

        ## labels to transfer from the kubernetes service to the target
        ##
        # targetLabels: ""

        ## Label selector for services to which this ServiceMonitor applies
        ##
        # selector: {}

        ## Namespaces from which services are selected
        ##
        # namespaceSelector:
        ## Match any namespace
        ##
        # any: false

        ## Explicit list of namespace names to select
        ##
        # matchNames: []

        ## Endpoints of the selected service to be monitored
        ##
        # endpoints: []
        ## Name of the endpoint's service port
        ## Mutually exclusive with targetPort
        # - port: ""

        ## Name or number of the endpoint's target port
        ## Mutually exclusive with port
        # - targetPort: ""

        ## File containing bearer token to be used when scraping targets
        ##
        #   bearerTokenFile: ""

        ## Interval at which metrics should be scraped
        ##
        #   interval: 30s

        ## HTTP path to scrape for metrics
        ##
        #   path: /metrics

        ## HTTP scheme to use for scraping
        ##
        #   scheme: http

        ## TLS configuration to use when scraping the endpoint
        ##
        #   tlsConfig:

        ## Path to the CA file
        ##
        # caFile: ""

        ## Path to client certificate file
        ##
        # certFile: ""

        ## Skip certificate verification
        ##
        # insecureSkipVerify: false

        ## Path to client key file
        ##
        # keyFile: ""

        ## Server name used to verify host name
      ##
      # serverName: ""
