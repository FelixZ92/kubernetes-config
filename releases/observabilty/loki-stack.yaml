---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: loki-stack
  namespace: observabilty
  annotations:
    fluxcd.io/automated: "false"
spec:
  releaseName: observabilty
  chart:
    repository: https://grafana.github.io/loki/charts
    name: loki-stack
    version: 0.24.0
  values:
    loki:
      enabled: true
      ## Affinity for pod assignment
      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
      affinity: {}
      # podAntiAffinity:
      #   requiredDuringSchedulingIgnoredDuringExecution:
      #   - labelSelector:
      #       matchExpressions:
      #       - key: app
      #         operator: In
      #         values:
      #         - loki
      #     topologyKey: "kubernetes.io/hostname"

      ## StatefulSet annotations
      annotations: {}

      # enable tracing for debug, need install jaeger and specify right jaeger_agent_host
      tracing:
        jaegerAgentHost:

      config:
        auth_enabled: false
        ingester:
          chunk_idle_period: 3m
          chunk_block_size: 262144
          chunk_retain_period: 1m
          lifecycler:
            ring:
              kvstore:
                store: inmemory
              replication_factor: 1

            ## Different ring configs can be used. E.g. Consul
            # ring:
            #   store: consul
            #   replication_factor: 1
            #   consul:
            #     host: "consul:8500"
            #     prefix: ""
            #     httpclienttimeout: "20s"
            #     consistentreads: true
        limits_config:
          enforce_metric_name: false
          reject_old_samples: true
          reject_old_samples_max_age: 168h
        schema_config:
          configs:
            - from: 2018-04-15
              store: boltdb
              object_store: filesystem
              schema: v9
              index:
                prefix: index_
                period: 168h
        server:
          http_listen_port: 3100
        storage_config:
          boltdb:
            directory: /data/loki/index
          filesystem:
            directory: /data/loki/chunks
        chunk_store_config:
          max_look_back_period: 0
        table_manager:
          retention_deletes_enabled: false
          retention_period: 0

      image:
        repository: grafana/loki
        tag: v1.2.0
        pullPolicy: IfNotPresent

      ## Additional Loki container arguments, e.g. log level (debug, info, warn, error)
      extraArgs: {}
      # log.level: debug

      livenessProbe:
        httpGet:
          path: /ready
          port: http-metrics
        initialDelaySeconds: 45

      ## ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
      networkPolicy:
        enabled: false

      ## The app name of loki clients
      client: {}
      # name:

      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
      nodeSelector: {}

      ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
      ## If you set enabled as "True", you need :
      ## - create a pv which above 10Gi and has same namespace with loki
      ## - keep storageClassName same with below setting
      persistence:
        enabled: false
        accessModes:
          - ReadWriteOnce
        size: 10Gi
        annotations: {}
        # subPath: ""
        # existingClaim:

      ## Pod Labels
      podLabels: {}

      ## Pod Annotations
      podAnnotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "http-metrics"

      podManagementPolicy: OrderedReady

      ## Assign a PriorityClassName to pods if set
      # priorityClassName:

      rbac:
        create: true
        pspEnabled: true

      readinessProbe:
        httpGet:
          path: /ready
          port: http-metrics
        initialDelaySeconds: 45

      replicas: 1

      resources: {}
      # limits:
      #   cpu: 200m
      #   memory: 256Mi
      # requests:
      #   cpu: 100m
      #   memory: 128Mi

      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001

      service:
        type: ClusterIP
        nodePort:
        port: 3100

      serviceAccount:
        create: true

      terminationGracePeriodSeconds: 4800

      ## Tolerations for pod assignment
      ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
      tolerations: []

      # The values to set in the PodDisruptionBudget spec
      # If not set then a PodDisruptionBudget will not be created
      podDisruptionBudget: {}
      # minAvailable: 1
      # maxUnavailable: 1

      updateStrategy:
        type: RollingUpdate

      serviceMonitor:
        enabled: true
        interval: ""
        additionalLabels: {}
        # scrapeTimeout: 10s

    promtail:
      enabled: true
      ## Affinity for pod assignment
      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
      affinity: {}

      annotations: {}

      deploymentStrategy: RollingUpdate

      image:
        repository: grafana/promtail
        tag: v1.2.0
        pullPolicy: IfNotPresent

      livenessProbe: {}

      loki:
        serviceName: ""  # Defaults to "${RELEASE}-loki" if not set
        servicePort: 3100
        serviceScheme: http
        # user: user
        # password: pass

      nameOverride: promtail

      ## Node labels for pod assignment
      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
      nodeSelector: {}

      pipelineStages:
        - docker: {}

      ## Pod Labels
      podLabels: {}

      podAnnotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "http-metrics"

      ## Assign a PriorityClassName to pods if set
      # priorityClassName:

      rbac:
        create: true
        pspEnabled: true

      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1

      resources: {}
      #  limits:
      #    cpu: 200m
      #    memory: 128Mi
      #  requests:
      #    cpu: 100m
      #    memory: 128Mi

      # Custom scrape_configs to override the default ones in the configmap
      scrapeConfigs: []

      # Custom scrape_configs together with the default ones in the configmap
      extraScrapeConfigs: []

      securityContext:
        readOnlyRootFilesystem: true
        runAsGroup: 0
        runAsUser: 0

      serviceAccount:
        create: true
        name:

      ## Tolerations for pod assignment
      ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
      tolerations:
        - key: node-role.kubernetes.io/master
          operator: Exists
          effect: NoSchedule

      # Extra volumes to scrape logs from
      volumes:
        - name: docker
          hostPath:
            path: /var/lib/docker/containers
        - name: pods
          hostPath:
            path: /var/log/pods

      volumeMounts:
        - name: docker
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: pods
          mountPath: /var/log/pods
          readOnly: true

      config:
        client:
          # Maximum wait period before sending batch
          batchwait: 1s
          # Maximum batch size to accrue before sending, unit is byte
          batchsize: 102400

          # Maximum time to wait for server to respond to a request
          timeout: 10s

          backoff_config:
            # Initial backoff time between retries
            minbackoff: 100ms
            # Maximum backoff time between retries
            maxbackoff: 5s
            # Maximum number of retries when sending batches, 0 means infinite retries
            maxretries: 20

          # The labels to add to any time series or alerts when communicating with loki
          external_labels: {}

        server:
          http_listen_port: 3101
        positions:
          filename: /run/promtail/positions.yaml
        target_config:
          # Period to resync directories being watched and files being tailed
          sync_period: 10s

      serviceMonitor:
        enabled: true
        interval: ""
        additionalLabels: {}
        # scrapeTimeout: 10s

    fluent-bit:
      enabled: false

    grafana:
      enabled: true

    prometheus:
      enabled: false
