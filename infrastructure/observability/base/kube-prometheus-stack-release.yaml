apiVersion: source.toolkit.fluxcd.io/v1beta1
kind: HelmRepository
metadata:
  name: prometheus-community
  namespace: monitoring
spec:
  interval: 10m
  url: https://prometheus-community.github.io/helm-charts
---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 5m
  chart:
    spec:
      chart: kube-prometheus-stack
      version: '18.0.0'
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: monitoring
      interval: 10m
  values:
    # Default values for kube-prometheus-stack.
    # This is a YAML-formatted file.
    # Declare variables to be passed into your templates.

    ## Provide a name in place of kube-prometheus-stack for `app:` labels
    ##
    nameOverride: ""

    ## Override the deployment namespace
    ##
    namespaceOverride: ""

    ## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.16.6
    ##
    kubeTargetVersionOverride: ""

    ## Allow kubeVersion to be overridden while creating the ingress
    ##
    kubeVersionOverride: ""

    ## Provide a name to substitute for the full names of resources
    ##
    fullnameOverride: ""

    ## Labels to apply to all resources
    ##
    commonLabels:
      monitoring: kube-prometheus-stack
    # scmhash: abc123
    # myLabel: aakkmd

    ## Create default rules for monitoring the cluster
    ##
    defaultRules:
      create: true
      rules:
        alertmanager: true
        etcd: true
        general: true
        k8s: true
        kubeApiserver: true
        kubeApiserverAvailability: true
        kubeApiserverError: true
        kubeApiserverSlos: true
        kubelet: true
        kubePrometheusGeneral: true
        kubePrometheusNodeAlerting: true
        kubePrometheusNodeRecording: true
        kubernetesAbsent: true
        kubernetesApps: true
        kubernetesResources: true
        kubernetesStorage: true
        kubernetesSystem: true
        kubeScheduler: true
        kubeStateMetrics: true
        network: true
        node: true
        prometheus: true
        prometheusOperator: true
        time: true

      ## Runbook url prefix for default rules
      runbookUrl: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#
      ## Reduce app namespace alert scope
      appNamespacesTarget: ".*"

      ## Labels for default rules
      labels: { }
      ## Annotations for default rules
      annotations: { }

      ## Additional labels for PrometheusRule alerts
      additionalRuleLabels: { }

    ## Deprecated way to provide custom recording or alerting rules to be deployed into the cluster.
    ##
    # additionalPrometheusRules: []
    #  - name: my-rule-file
    #    groups:
    #      - name: my_group
    #        rules:
    #        - record: my_record
    #          expr: 100 * my_record

    ## Provide custom recording or alerting rules to be deployed into the cluster.
    ##
    additionalPrometheusRulesMap: { }
    #  rule-name:
    #    groups:
    #    - name: my_group
    #      rules:
    #      - record: my_record
    #        expr: 100 * my_record

    ##
    global:
      rbac:
        create: true
        pspEnabled: true
        pspAnnotations: { }
          ## Specify pod annotations
          ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
          ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
          ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl
          ##
          # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
        # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
        # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'

      ## Reference to one or more secrets to be used when pulling images
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
      ##
      imagePullSecrets: [ ]
      # - name: "image-pull-secret"

    ## Configuration for alertmanager
    ## ref: https://prometheus.io/docs/alerting/alertmanager/
    ##
    alertmanager:

      ## Deploy alertmanager
      ##
      enabled: true

      ## Annotations for Alertmanager
      ##
      annotations: { }

      ## Api that prometheus will use to communicate with alertmanager. Possible values are v1, v2
      ##
      apiVersion: v2

      ## Service account for Alertmanager to use.
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
      ##
      serviceAccount:
        create: true
        name: ""
        annotations: { }

      ## Configure pod disruption budgets for Alertmanager
      ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
      ## This configuration is immutable once created and will require the PDB to be deleted to be changed
      ## https://github.com/kubernetes/kubernetes/issues/45398
      ##
      podDisruptionBudget:
        enabled: true
        minAvailable: 1
        maxUnavailable: ""

      ## Alertmanager configuration directives
      ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
      ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
      ##
      config:
        global:
          resolve_timeout: 5m
        route:
          group_by: [ 'job' ]
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 12h
          receiver: 'slack'
          routes:
            - match:
                alertname: Watchdog
              receiver: 'null'
        receivers:
          - name: 'null'
          - name: 'slack'
            slack_configs:
              - api_url: https://hooks.slack.com/services/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
                channel: '#alerts'
                send_resolved: true

      ingress:
        enabled: false

        # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
        # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
        # ingressClassName: nginx

        annotations: { }

        labels: { }

        ## Hosts must be provided if Ingress is enabled.
        ##
        hosts: [ ]
        # - alertmanager.domain.com

        ## Paths to use for ingress rules - one path should match the alertmanagerSpec.routePrefix
        ##
        paths: [ ]
        # - /

        ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
        ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
        # pathType: ImplementationSpecific

        ## TLS configuration for Alertmanager Ingress
        ## Secret must be manually created in the namespace
        ##
        tls: [ ]
        # - secretName: alertmanager-general-tls
        #   hosts:
        #   - alertmanager.example.com

      ## Configuration for Alertmanager secret
      ##
      secret:
        annotations: { }

      ## Configuration for creating an Ingress that will map to each Alertmanager replica service
      ## alertmanager.servicePerReplica must be enabled
      ##
      ingressPerReplica:
        enabled: false

        # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
        # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
        # ingressClassName: nginx

        annotations: { }
        labels: { }

        ## Final form of the hostname for each per replica ingress is
        ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}
        ##
        ## Prefix for the per replica ingress that will have `-$replicaNumber`
        ## appended to the end
        hostPrefix: ""
        ## Domain that will be used for the per replica ingress
        hostDomain: ""

        ## Paths to use for ingress rules
        ##
        paths: [ ]
        # - /

        ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
        ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
        # pathType: ImplementationSpecific

        ## Secret name containing the TLS certificate for alertmanager per replica ingress
        ## Secret must be manually created in the namespace
        tlsSecretName: ""

        ## Separated secret for each per replica Ingress. Can be used together with cert-manager
        ##
        tlsSecretPerReplica:
          enabled: false
          ## Final form of the secret for each per replica ingress is
          ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}
          ##
          prefix: "alertmanager"

      ## Configuration for Alertmanager service
      ##
      service:
        annotations: { }
        labels: { }
        clusterIP: ""

        ## Port for Alertmanager Service to listen on
        ##
        port: 9093
        ## To be used with a proxy extraContainer port
        ##
        targetPort: 9093
        ## Port to expose on each node
        ## Only used if service.type is 'NodePort'
        ##
        nodePort: 30903
        ## List of IP addresses at which the Prometheus server service is available
        ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
        ##

        ## Additional ports to open for Alertmanager service
        additionalPorts: [ ]

        externalIPs: [ ]
        loadBalancerIP: ""
        loadBalancerSourceRanges: [ ]
        ## Service type
        ##
        type: ClusterIP

      ## Configuration for creating a separate Service for each statefulset Alertmanager replica
      ##
      servicePerReplica:
        enabled: false
        annotations: { }

        ## Port for Alertmanager Service per replica to listen on
        ##
        port: 9093

        ## To be used with a proxy extraContainer port
        targetPort: 9093

        ## Port to expose on each node
        ## Only used if servicePerReplica.type is 'NodePort'
        ##
        nodePort: 30904

        ## Loadbalancer source IP ranges
        ## Only used if servicePerReplica.type is "LoadBalancer"
        loadBalancerSourceRanges: [ ]
        ## Service type
        ##
        type: ClusterIP

      ## If true, create a serviceMonitor for alertmanager
      ##
      serviceMonitor:
        ## Scrape interval. If not set, the Prometheus default scrape interval is used.
        ##
        interval: ""
        selfMonitor: true

        ## proxyUrl: URL of a proxy that should be used for scraping.
        ##
        proxyUrl: ""

        ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
        scheme: ""

        ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
        ## Of type: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#tlsconfig
        tlsConfig: { }

        bearerTokenFile:

        ## Metric relabel configs to apply to samples before ingestion.
        ##
        metricRelabelings: [ ]
        # - action: keep
        #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
        #   sourceLabels: [__name__]

        #   relabel configs to apply to samples before ingestion.
        ##
        relabelings: [ ]
        # - sourceLabels: [__meta_kubernetes_pod_node_name]
        #   separator: ;
        #   regex: ^(.*)$
        #   targetLabel: nodename
        #   replacement: $1
        #   action: replace

      ## Settings affecting alertmanagerSpec
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#alertmanagerspec
      ##
      alertmanagerSpec:
        replicas: 2
        retention: 120h
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: local-path
              accessModes: [ "ReadWriteOnce" ]
              resources:
                requests:
                  storage: 2700Mi
        externalUrl: https://alertmanager
        routePrefix: /
        nodeSelector:
          storage: local

        resources:
          requests:
            memory: "200Mi"
            cpu: "100m"
          limits:
            memory: "300Mi"
            cpu: "200m"
        priorityClassName: ""


    ## Using default values from https://github.com/helm/charts/blob/master/stable/grafana/values.yaml
    ##
    grafana:
      rbac:
        pspUseAppArmor: false
      enabled: true
      ## Deploy default dashboards.
      ##
      defaultDashboardsEnabled: true

      # Created and sealed from passwordstore via 'make create-grafana-secret'
      admin:
        existingSecret: "grafana-admin-user"
        userKey: username
        passwordKey: password

      envFromSecret: "grafana-generic-oauth"

      env:
        GF_SERVER_ROOT_URL: "https://grafana.${BASE_DOMAIN}"
        GF_AUTH_GITLAB_ENABLED: "true"
        GF_AUTH_GITLAB_ALLOW_SIGN_UP: "false"
        GF_AUTH_GITLAB_SCOPES: "read_api"
        GF_AUTH_GITLAB_AUTH_URL: "https://gitlab.com/oauth/authorize"
        GF_AUTH_GITLAB_TOKEN_URL: "https://gitlab.com/oauth/token"
        GF_AUTH_GITLAB_API_URL: "https://gitlab.com/api/v4"
        GF_AUTH_GITLAB_ALLOWED_GROUPS: "fzx"
        GF_AUTH_GITLAB_ROLE_ATTRIBUTE_PATH: "is_admin && 'Admin' || 'Viewer'"

      plugins:
        - grafana-piechart-panel
        - grafana-clock-panel
      dashboards:
        default:
          ceph-cluster:
            gnetId: 2842
            revision: 14
            datasource: Prometheus
          ceph-ods:
            gnetId: 5336
            revision: 5
            datasource: Prometheus
          ceph-pools:
            gnetId: 5342
            revision: 5
            datasource: Prometheus
      resources:
        requests:
          cpu: 50m
          memory: 128Mi
        limits:
          cpu: 100m
          memory: 256Mi

      sidecar:
        dashboards:
          searchNamespace: "ALL"
        resources:
          requests:
            cpu: 20m
            memory: 64Mi
          limits:
            cpu: 40m
            memory: 128Mi

      extraConfigmapMounts: [ ]
      # - name: certs-configmap
      #   mountPath: /etc/grafana/ssl/
      #   configMap: certs-configmap
      #   readOnly: true

      # TODO use this for Loki
      ## Configure additional grafana datasources (passed through tpl)
      ## ref: http://docs.grafana.org/administration/provisioning/#datasources
      additionalDataSources:
        - name: Loki
          type: loki
          access: proxy
          url: http://loki.loki.svc:3100
          version: 1
      # - name: prometheus-sample
      #   access: proxy
      #   basicAuth: true
      #   basicAuthPassword: pass
      #   basicAuthUser: daco
      #   editable: false
      #   jsonData:
      #       tlsSkipVerify: true
      #   orgId: 1
      #   type: prometheus
      #   url: https://{{ printf "%s-prometheus.svc" .Release.Name }}:9090
      #   version: 1
      downloadDashboards:
        resources:
          requests:
            cpu: 20m
            memory: 64Mi
          limits:
            cpu: 40m
            memory: 128Mi
    ## Component scraping the kube controller manager
    ##
    kubeControllerManager:
      enabled: true
      endpoints: []
      # - 10.42.0.1
      # - 10.141.4.23
      # - 10.141.4.24

    ## Component scraping coreDns. Use either this or kubeDns
    ##
    coreDns:
      enabled: true

    kubeEtcd:
      enabled: true
      endpoints: []
      # - 10.42.0.1
      # - 10.141.4.23
      # - 10.141.4.24

    kubeScheduler:
      enabled: true
      endpoints: []
#        - 10.42.0.1
      # - 10.141.4.23
      # - 10.141.4.24

    kubeProxy:
      enabled: true
      endpoints: []
#        - 192.168.0.11
#        - 192.168.0.12
      # - 10.141.4.23
      # - 10.141.4.24

    ## Manages Prometheus and Alertmanager components
    ##
    prometheusOperator:
      enabled: true

      # Prometheus-Operator v0.39.0 and later support TLS natively.
      tls:
        enabled: true
        # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants
        tlsMinVersion: VersionTLS13

      admissionWebhooks:
        patch:
          enabled: true
          resources: { }

      resources:
        requests:
          cpu: 50m
          memory: 128Mi
        limits:
          cpu: 100m
          memory: 256Mi

    ## Deploy a Prometheus instance
    ##
    prometheus:
      enabled: true
      podDisruptionBudget:
        enabled: true
        minAvailable: 1
        maxUnavailable: ""
      resources:
        limits:
          cpu: 200m
          memory: 4Gi
        requests:
          cpu: 100m
          memory: 2Gi

      prometheusSpec:
        externalUrl: "https://prometheus"

        nodeSelector:
          storage: local
        retention: 10d
        retentionSize: ""
        replicas: 2

        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: local-path
              accessModes: [ "ReadWriteOnce" ]
              resources:
                requests:
                  storage: 2700Mi
        priorityClassName: ""

        resources:
          requests:
            cpu: 100m
            memory: 500Mi
          limits:
            cpu: 200m
            memory: 1Gi

        ruleSelector:
          matchLabels:
            monitoring: kube-prometheus-stack
        serviceMonitorSelector:
          matchLabels:
            monitoring: kube-prometheus-stack
        podMonitorSelector:
          matchLabels:
            monitoring: kube-prometheus-stack
        probeSelector:
          matchLabels:
            monitoring: kube-prometheus-stack
    prometheus-node-exporter:
      resources:
        limits:
          cpu: 200m
          memory: 50Mi
        requests:
          cpu: 100m
          memory: 30Mi

    kube-state-metrics:
      resources:
        requests:
          cpu: 20m
          memory: 64Mi
        limits:
          cpu: 40m
          memory: 128Mi
